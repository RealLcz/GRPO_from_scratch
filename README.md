# GRPO from scratch
This repo contains codes and training results of a pure pytorch implementation of GRPO. 

To run my code, you may first set up the environment as required :

```pip install -r requirements.txt```

After that, you could directly use script :

```python3 standard.py```

## Training settings

### Foundation Models
Qwen/Qwen2.5-1.5B-Instruct  [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-FFD21E?logo=huggingface&logoColor=000)](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)


### Training Benchmark
swulling/gsm8k_chinese  [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97-Hugging%20Face-FFD21E?logo=huggingface&logoColor=000)](https://huggingface.co/datasets/swulling/gsm8k_chinese)


## Training Results
Digit Reward Mean
<img width="2528" height="1328" alt="W B Chart 2026_2_13 02_36_10" src="https://github.com/user-attachments/assets/c5f93422-e61c-419f-bb5c-c3f4b603c4da" />
Mark Reward Mean
<img width="2528" height="1328" alt="W B Chart 2026_2_13 02_35_53" src="https://github.com/user-attachments/assets/d0ff2a79-0253-4ad6-a220-33c527f33784" />
